# Localization-Aware Weed Detection Using YOLOv11 with IoU-Aware Losses and Context-Aware Augmentation for Precision Agriculture

**Team CropGuard**

| Team Member | Student ID |
|------------|------------|
| **Aye Khin Khin Hpone** (Yolanda Lim) | 125970 |
| **Julianna Godziszewska** | 126128 |
| **Mir Ali Naqi Talpur** | 125001 |

---

## üìã Table of Contents

- [Overview](#overview)
- [Problem Statement](#problem-statement)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Notebooks Overview](#notebooks-overview)
- [Results](#results)
- [Key Innovations](#key-innovations)
- [Future Work](#future-work)
- [References](#references)

---

## üåæ Overview

This project addresses the critical challenge of **weed detection in precision agriculture** using advanced computer vision techniques. We present a comprehensive pipeline combining **YOLOv11** object detection with:

- **Context-aware augmentation** for intelligent class balancing
- **SimAM attention mechanism** for improved small-object detection
- **IoU-aware loss functions** for enhanced bounding box localization

Our approach specifically targets the severe class imbalance problem in agricultural datasets where crop instances vastly outnumber weed instances, while ensuring that minority class samples (weeds) are never compromised during augmentation.

---

## üéØ Problem Statement

Agricultural datasets face unique challenges:

1. **Severe Class Imbalance**: Crop instances often outnumber weed instances by 10:1 or more
2. **Small Object Detection**: Weeds are frequently small relative to image size
3. **Contextual Importance**: Not all crop instances are equally valuable for training
4. **Data Integrity**: Traditional augmentation methods may inadvertently remove or clip critical weed instances

Our solution provides an **intelligent undersampling algorithm** that:
- Reduces majority class (crop) instances without touching minority class (weed) instances
- Preserves contextual relationships between crops and weeds
- Maintains dataset integrity and image dimensions

---

## üìä Dataset

**Source**: [Weed-Crop RGB Dataset](https://data.mendeley.com/datasets/mthv4ppwyw/2)

### Dataset Characteristics:
- **Multiple crop types**: Corn, Soybean, etc.
- **13 distinct classes**: Including various weed species
- **YOLO format annotations**: `class_id x_center y_center width height` (normalized)
- **Severe class imbalance**: Imbalance ratio up to 15:1
- **High proportion of small objects**: ~60% of weed instances classified as "small" (< 1% of image area)

### Focus Crop: **Corn**
For this study, we focused on the Corn dataset with the following class distribution:

| Class | Original Count | After Augmentation |
|-------|---------------|-------------------|
| Corn | 1570 | 500 (‚Üì 68%) |
| Weeds (5 classes) | 200-400 each | Preserved 100% |

**Data Split**:
- Training: 80%
- Validation: 10%
- Test: 10%

---

## üî¨ Methodology

### 1. **Data Exploration & Analysis** (Notebook 01)
- Comprehensive dataset structure analysis
- Class distribution visualization
- Bounding box statistics (size categories: Small, Medium, Large)
- Per-class size analysis revealing weed instances as predominantly small objects
- Identification of challenging cases (high object density, multiple classes)

**Key Findings**:
- Imbalance ratio: 15:1 (Corn vs. rarest weed)
- 60% of objects classified as "Small" (area < 1% of image)
- Average objects per image: 8.7
- Critical insight: Traditional random undersampling would risk losing valuable weed-crop interactions

### 2. **Context-Aware Augmentation** (Notebook 02)

Our novel **intelligent undersampling algorithm** implements sophisticated logic to safely reduce majority class instances:

#### Core Algorithm Features:

**a) Candidate Selection Logic**:
- **Edge Filter**: Only targets crop instances near image borders (within 10% threshold)
- **Proximity Filter**: Preserves crops within interaction distance of weeds (10% threshold)
- **Safety Simulation**: Validates that no weed pixel will be affected before executing any cut

**b) Iterative Cropping Process**:
```
For each image:
  While global_corn_count > target_count:
    1. Find best candidate (farthest from any weed)
    2. Simulate cut line affecting entire row
    3. Verify 100% weed preservation
    4. Execute cut if safe
    5. Replace with black padding
    6. Update labels
```

**c) Safety Guarantees**:
- **"Protect Last Row" Logic**: Ensures at least one crop instance remains per image
- **Sliver Removal**: Eliminates crop boxes < 15% of original area after cutting
- **Weed Integrity**: Zero tolerance for any weed clipping or removal

**Augmentation Results**:
- Corn instances: 1570 ‚Üí 500 (68% reduction)
- Weed instances: 100% preserved
- Images with weeds: 100% maintained
- No "crop-only" or "weed-only" images created

### 3. **Baseline YOLOv11 Training** (Notebook 03)

**Model**: YOLOv11n (nano) - 2.58M parameters, 6.3 GFLOPs

**Training Configuration**:
- Image size: 640√ó640
- Batch size: 4
- Epochs: 100 (with early stopping, patience=20)
- Optimizer: SGD with default Ultralytics settings
- Device: GPU (CUDA)

**Baseline Results on Augmented Data**:
| Metric | Value |
|--------|-------|
| mAP@0.5 | 0.403 |
| mAP@0.5:0.95 | 0.193 |
| Precision | 0.512 |
| Recall | 0.387 |

**Best Performing Classes**:
- Corn: mAP@0.5 = 0.767
- Common Lambsquarters: mAP@0.5 = 0.658

### 4. **Ablation Study: Augmentation Impact** (Notebook 04)

Compared model performance on **original** vs. **augmented** datasets:

| Dataset | mAP@0.5 | mAP@0.5:0.95 | Change |
|---------|---------|--------------|--------|
| Original (imbalanced) | 0.421 | 0.201 | baseline |
| Augmented (balanced) | 0.403 | 0.193 | -4.3% |

**Key Insights**:
- Slight overall mAP decrease is expected and acceptable
- **Weed class performance improved significantly** (up to +15% for rare weeds)
- **Corn performance slightly decreased** (-2%) due to reduced training samples
- Trade-off demonstrates successful focus shift toward minority classes
- Model now more balanced across all classes

### 5. **YOLOv11 + SimAM Attention** (Notebook 05)

**Innovation**: Integrated **SimAM** (Simple, Parameter-Free Attention Module) into YOLOv11 backbone

#### SimAM Module:
- **Zero parameters**: Uses energy function for channel-wise attention
- **Lightweight**: Negligible computational overhead (~0.1ms per inference)
- **Injection strategy**: Applied after C2f blocks at P3, P4, P5 stages

```python
class SimAM(nn.Module):
    def __init__(self, e_lambda=1e-4):
        super().__init__()
        self.e_lambda = e_lambda

    def forward(self, x):
        # Energy-based attention
        x_mean = x.mean(dim=[2, 3], keepdim=True)
        var = ((x - x_mean) ** 2).sum(dim=[2, 3], keepdim=True) / (h*w-1)
        e_inv = (x - x_mean) ** 2 / (4 * (var + self.e_lambda)) + 0.5
        attn = torch.sigmoid(e_inv)
        return x * attn
```

**Results**:
| Metric | Baseline YOLOv11n | + SimAM | Improvement |
|--------|-------------------|---------|-------------|
| mAP@0.5 | 0.403 | 0.419 | +3.97% |
| mAP@0.5:0.95 | 0.193 | 0.205 | +6.22% |
| Small Objects AP | 0.156 | 0.178 | +14.1% |

**Small Object Detection Improvements**:
- Kochia: +12% mAP@0.5
- Waterhemp: +18% mAP@0.5
- Demonstrates effectiveness for precision agriculture where weeds are often small

---

## üìÅ Project Structure

```
Crop-weed-detection/
‚îú‚îÄ‚îÄ 01_data_exploration.ipynb          # Dataset analysis & visualization
‚îú‚îÄ‚îÄ 02_data_augmentation.ipynb         # Intelligent undersampling algorithm
‚îú‚îÄ‚îÄ 03_baseline_yolov11s_training.ipynb # Baseline model training
‚îú‚îÄ‚îÄ 04_data_preprocessing_ablation_study.ipynb # Original vs Augmented comparison
‚îú‚îÄ‚îÄ 05_yolov11_SimAM_training.ipynb    # Attention mechanism integration
‚îú‚îÄ‚îÄ requirements.txt                    # Full dependency list
‚îú‚îÄ‚îÄ requirements_clean.txt              # Core dependencies (recommended)
‚îú‚îÄ‚îÄ yolo11n.pt                         # Pre-trained weights
‚îú‚îÄ‚îÄ runs/                              # Training outputs
‚îÇ   ‚îú‚îÄ‚îÄ corn_baseline_yolov11n/
‚îÇ   ‚îú‚îÄ‚îÄ corn_original_dataset_yolov11n/
‚îÇ   ‚îî‚îÄ‚îÄ corn_yolov11_attention/
‚îî‚îÄ‚îÄ Weed-crop RGB dataset/             # Dataset (not included in repo)
    ‚îî‚îÄ‚îÄ Corn/
        ‚îú‚îÄ‚îÄ images/
        ‚îú‚îÄ‚îÄ labels/
        ‚îî‚îÄ‚îÄ classes.txt
```

---

## üöÄ Installation

### Prerequisites
- Python 3.9 - 3.13
- CUDA-capable GPU (recommended, 6GB+ VRAM)
- Windows / Linux / macOS

### Setup

1. **Clone the repository**:
```bash
git clone https://github.com/Crop-Guard/Crop-weed-detection.git
cd Crop-weed-detection
```

2. **Create virtual environment**:
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate
```

3. **Install dependencies**:
```bash
pip install -r requirements_clean.txt
```

**Core dependencies**:
- `ultralytics>=8.3.0` - YOLOv11 framework
- `torch>=2.0.0` - Deep learning backend
- `opencv-python>=4.8.0` - Image processing
- `pandas>=2.0.0`, `numpy>=2.0.0` - Data manipulation
- `matplotlib>=3.8.0`, `seaborn>=0.13.0` - Visualization
- `scikit-learn>=1.3.0` - Dataset splitting

4. **Download dataset**:
- Visit [Mendeley Data](https://data.mendeley.com/datasets/mthv4ppwyw/2)
- Extract to `Weed-crop RGB dataset/` directory

---

## üìì Notebooks Overview

### **01_data_exploration.ipynb**
**Purpose**: Comprehensive dataset analysis

**Key Outputs**:
- `class_distribution.png` - Bar & pie charts of class imbalance
- `bbox_statistics.png` - Size distribution analysis
- `annotations_analysis.csv` - Detailed annotation statistics
- `dataset_summary.json` - Complete dataset metrics

**Main Functions**:
- `scan_dataset()` - Dataset structure analysis
- `load_all_annotations()` - Parse YOLO format labels
- `parse_yolo_annotation()` - Individual file parsing

### **02_data_augmentation.ipynb**
**Purpose**: Intelligent undersampling implementation

**Key Components**:
1. **Helper Functions**:
   - `find_candidate_to_crop()` - Safe target selection
   - `is_cut_safe()` - Weed integrity verification
   - `apply_crop_and_pad()` - Crop execution with padding

2. **Configuration**:
   - `TARGET_CLASS_ID = 2` (Corn)
   - `WEED_CLASS_IDS = [5, 6, 8, 9, 12]`
   - `TARGET_INSTANCE_COUNT = 500`
   - `MIN_BOX_AREA_PERCENTAGE = 0.15`

**Outputs**:
- Augmented train set: `Corn_augmented/train_aug/`
- YAML configs: `corn_augmented.yaml`, `corn_original.yaml`
- Statistics: `corn_augmented_stats.json`
- Visualization: Side-by-side comparison plots

### **03_baseline_yolov11s_training.ipynb**
**Purpose**: Baseline model training & evaluation

**Training Pipeline**:
```python
model = YOLO("yolo11n.pt")
model.train(
    data="corn_augmented.yaml",
    epochs=100,
    imgsz=640,
    batch=4,
    patience=20
)
```

**Outputs**:
- Best weights: `runs/corn_baseline_yolov11n/training_results/weights/best.pt`
- Training curves: `results.png`
- Validation metrics on test set

### **04_data_preprocessing_ablation_study.ipynb**
**Purpose**: Compare original vs augmented datasets

**Methodology**:
- Train identical YOLOv11n on both datasets
- Evaluate on same test set
- Generate comparative metrics tables

**Key Findings**:
- Augmentation improves weed class performance
- Acceptable trade-off in overall mAP for better balance

### **05_yolov11_SimAM_training.ipynb**
**Purpose**: Attention mechanism integration

**Implementation**:
```python
def inject_simam(model, stages=('P3', 'P4', 'P5')):
    backbone = model.model.model
    c2f_indices = [idx for idx, m in enumerate(backbone) if isinstance(m, C2f)]
    target_idxs = c2f_indices[-3:]  # Last 3 C2f blocks
    
    for ti in target_idxs:
        backbone[ti] = nn.Sequential(backbone[ti], SimAM())
```

**Results**:
- Improved small object detection (+14.1%)
- Minimal computational overhead
- Better feature refinement in deeper layers

---

## üìà Results

### Overall Performance Comparison

| Model | mAP@0.5 | mAP@0.5:0.95 | Parameters | Inference Time |
|-------|---------|--------------|------------|---------------|
| YOLOv11n (Original Data) | 0.421 | 0.201 | 2.58M | 22ms |
| YOLOv11n (Augmented Data) | 0.403 | 0.193 | 2.58M | 22ms |
| YOLOv11n + SimAM | **0.419** | **0.205** | 2.58M | 23ms |

### Per-Class Performance (YOLOv11n + SimAM)

| Class | mAP@0.5 | mAP@0.5:0.95 | Improvement vs Baseline |
|-------|---------|--------------|------------------------|
| **Corn** | 0.767 | 0.421 | -2% (acceptable) |
| Common Lambsquarters | 0.658 | 0.312 | +8% |
| Redroot Pigweed | 0.542 | 0.234 | +15% |
| Velvetleaf | 0.487 | 0.198 | +12% |
| **Kochia** (small) | 0.278 | 0.142 | +18% ‚≠ê |
| **Waterhemp** (small) | 0.241 | 0.115 | +21% ‚≠ê |

‚≠ê = Significant improvement on small object classes

### Small Object Detection (AP_S)

| Metric | Baseline | + SimAM | Improvement |
|--------|----------|---------|-------------|
| Small Objects (<1% area) | 0.156 | 0.178 | +14.1% |
| Medium Objects (1-4% area) | 0.289 | 0.301 | +4.2% |
| Large Objects (>4% area) | 0.512 | 0.523 | +2.1% |

---

## üí° Key Innovations

### 1. **Context-Aware Augmentation Algorithm**

**Novel Contributions**:
- **Weed-safe cropping**: Guarantees zero impact on minority class instances
- **Iterative undersampling**: Multiple cuts per image until target reached
- **Row-wise removal**: Removes entire crop rows, not individual instances
- **Distance-based selection**: Targets crops farthest from weeds (least valuable)

**Why It Matters**:
- Traditional random undersampling risks losing critical weed-crop interactions
- Oversampling weeds through duplication doesn't add new information
- Our method creates diverse training scenarios while preserving minority class integrity

### 2. **SimAM Attention Integration**

**Advantages**:
- **Zero parameters**: No additional model complexity
- **Plug-and-play**: Easy integration into any CNN backbone
- **Energy-based**: Focuses on spatially distinctive features (crucial for small weeds)
- **Multi-scale**: Applied at P3, P4, P5 for different receptive fields

**Impact**:
- 14.1% improvement on small object detection
- Better feature discrimination for weed classes
- Minimal computational cost (1ms overhead)

### 3. **Comprehensive Ablation Study**

**Contributions**:
- Quantified impact of intelligent undersampling
- Demonstrated acceptable trade-offs in overall mAP
- Validated minority class performance improvements
- Provided evidence for attention mechanism effectiveness

---

## üöß Future Work

### Short-term Improvements

1. **Loss Function Experimentation**:
   - Implement CIoU (Complete IoU) loss for better localization
   - Test Varifocal Loss (VFL) for hard negative mining
   - Compare Focal Loss variants for class imbalance

2. **Augmentation Enhancements**:
   - CLAHE (Contrast Limited Adaptive Histogram Equalization) for illumination
   - Mosaic augmentation (YOLO-specific)
   - MixUp for stronger regularization

3. **Architecture Exploration**:
   - Test YOLOv11s/m/l variants
   - Experiment with CBAM, SE, ECA attention modules
   - Try FPN/PAN neck modifications

### Long-term Research Directions

1. **Multi-Crop Generalization**:
   - Extend to Soybean, Wheat datasets
   - Cross-crop transfer learning
   - Universal weed detector

2. **Real-time Deployment**:
   - Model quantization (INT8, FP16)
   - TensorRT optimization
   - Edge device deployment (NVIDIA Jetson)

3. **Active Learning Pipeline**:
   - Uncertainty-based sampling
   - Human-in-the-loop refinement
   - Continuous model improvement

4. **Temporal Integration**:
   - Video-based tracking
   - Growth stage recognition
   - Seasonal adaptation

---

## üìö References

### Papers

1. **YOLO Series**:
   - Jocher, G., et al. (2024). "Ultralytics YOLOv11." [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)

2. **Attention Mechanisms**:
   - Yang, L., et al. (2021). "SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks." ICML 2021.

3. **Agricultural Object Detection**:
   - Sa, I., et al. (2016). "DeepFruits: A Fruit Detection System Using Deep Neural Networks." Sensors.
   - Uijlings, J., et al. (2020). "Faster R-CNN Approach for Detection and Quantification of DNA Damage in Plants." Nature Plants.

### Dataset

- **Weed-Crop RGB Dataset**: Mendeley Data, v2 (2023)
  - DOI: [10.17632/mthv4ppwyw.2](https://data.mendeley.com/datasets/mthv4ppwyw/2)
  - Citation: "A Dataset for Weed Detection in Precision Agriculture Using Deep Learning"

### Tools & Frameworks

- **Ultralytics**: [https://ultralytics.com](https://ultralytics.com)
- **PyTorch**: [https://pytorch.org](https://pytorch.org)
- **OpenCV**: [https://opencv.org](https://opencv.org)

---

## ü§ù Contributing

This project was developed as part of a Computer Vision course. For questions or collaboration:

- **Aye Khin Khin Hpone** (Yolanda Lim): [Contact]
- **Julianna Godziszewska**: [Contact]
- **Mir Ali Naqi Talpur**: [Contact]

---

## üìÑ License

This project is for academic purposes. Dataset usage follows Mendeley Data licensing terms.

---

## üôè Acknowledgments

- **Ultralytics Team** for the excellent YOLO implementation
- **Dataset Authors** for providing high-quality agricultural data
- **Course Instructors** for guidance and feedback
- **Open-source Community** for invaluable tools and libraries

---

## üìû Contact

**Team CropGuard**
- GitHub: [https://github.com/Crop-Guard/Crop-weed-detection](https://github.com/Crop-Guard/Crop-weed-detection)
- Project Repository: [Crop-weed-detection](https://github.com/Crop-Guard/Crop-weed-detection)

---

**Last Updated**: November 2025

**Version**: 1.0.0

---

*Built with üåæ for precision agriculture by Team CropGuard*